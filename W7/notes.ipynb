{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning for Newbies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Video Game Playing Agent\n",
    "\n",
    "In this tutorial, we will build a learning agent to play video games. To follow this tutorial you will need to install the `gym` package provided by OpenAI. Please follow the installation instruction [here](https://gym.openai.com). Note we will only need the `atari` environment.\n",
    "\n",
    "In this project, I provide some templates to simplifying and unifying wrappers for gym-atari environment in `atari_a3c/atari.py`. Basicly, one \n",
    "- creates a game-environment using `env = create_env(GAME-NAME)`\n",
    "- then can ask `env` for the current observation of screen pixels as standard numpy array\n",
    "- and can given `env` an integer representing a button being pressed on a game console\n",
    "\n",
    "The task is to design an observation to action mapping, called a _policy_, so to win a game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Def\n",
    "We build a program that can play video games:\n",
    "```\n",
    "Input: Game-Environment (env)\n",
    "Output: Game-Policy (policy)\n",
    "```\n",
    "\n",
    "__env__:\n",
    "```\n",
    "Input: action (0~k, say, 2)\n",
    "Output: screen-image, reward, game-is-over\n",
    "```\n",
    "\n",
    "__policy__:\n",
    "```\n",
    "Input: screen-image\n",
    "Output: action\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start playing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see a concrete example of a policy. __REMINDER__: states are stacked-4 step observed and processed frames -- 42x42 pixels per frame, 1 or 3 channels (Mono/RGB colours) per pixel -- totally a state is a $\\Big[[12(=3\\times4) | 4] \\times 42 \\times 42\\Big]$ array. In decision-making scenarios, the output of a policy should be the likelihood of taking each possible actions given the observation.\n",
    "\n",
    "This is a good case to apply neural network models to process the states. \n",
    "\n",
    "* let's try to play with our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First check the state, i.e. the input to our model\n",
    "import torch\n",
    "from atari_a3c import atari\n",
    "env = atari.create_atari_env('PongDeterministic-v4')\n",
    "s_ = env.reset()\n",
    "print(s_.shape) # This is a 4-frame mono-colour observation. \n",
    "\n",
    "# the state s_ is now a numpy array, let's convert it to a tensor to process\n",
    "state_tensor = torch.from_numpy(s_).unsqueeze(0)\n",
    "# unsqueeze(0) add a \"batch\" dimension, i.e. in this batch, we only have 1 sample\n",
    "print(type(state_tensor))\n",
    "print(state_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Lets process the observation using convolutional neural networks\n",
    "-- We will worry about the decision making in a later stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "num_inputs = 4 # Corresponding to s_.shape[0]\n",
    "conv1 = nn.Conv2d(num_inputs, 32, 3, stride=2, padding=1)\n",
    "conv2 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "conv3 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "conv4 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "conv_layers = nn.Sequential(\n",
    "    conv1, nn.ELU(), \n",
    "    conv2, nn.ELU(), \n",
    "    conv3, nn.ELU(), \n",
    "    conv4, nn.ELU()) # means when processing an x\n",
    "# the computation is \n",
    "#  - elu(conv1(x)) -> x'\n",
    "#  - elu(conv2(x')) -> x''\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to process our `state-tensor` using the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = conv_layers(state_tensor)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good. The image is now processed, and we are about to make decisions out of these _features_ of the observation. The simplest way to make use of those features is to build linear models from them for each possible action's probability. To be specific, see how many features we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.view(1, -1).shape # 1: we have 1 sample; \n",
    "# -1 means unravelling all features per sample into one row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we will make a fully connected linear layer of $288 \\times 3$, where 3 outputs correspond to UP/DOWN/STAY actions. (Let's consider a simple Pingpong game for now. To play more sophisticated games, more actions are needed, of course.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_linear = nn.Linear(288, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_likelihood = actor_linear(y.view(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(action_likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks OK. The next steps are to\n",
    "- convert the likelihood to proper probability density -- so all actions have a probability between [0, 1] to be chosen, and all the probabilities adding to 1.0. We do this by using [softmax], which is often used in describing the probability of classes in multi-class classification problems.\n",
    "- draw a random sample from the above probability distribution\n",
    "- commit the action, and so we move to the next time step \n",
    "\n",
    "[softmax]: https://www.youtube.com/watch?v=LLux1SW--oM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_prob = nn.functional.softmax(action_likelihood, dim=1) \n",
    "print(action_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dim=1` means we perform softmax among all numbers in each ROW. This is not relevant in our case here -- we have only one ROW and it is meaningless to take softmax along a COLUMN. Generally speaking, the `dim` should refer to the dimension of an array, along which we want to get probability among the elements. E.g. here we want the probability among the ACTIONS, and all the actions of an observation (in this case we have only one) are collected in the corresponding ROW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = action_prob.multinomial(1) # take one sample according to the distribution\n",
    "print(action)\n",
    "\n",
    "# Try to run this cell MULTIPLE TIMES and see what you got."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets commit the selected action to the environment. Say\n",
    "- Action-0: UP\n",
    "- Action-1: STAY\n",
    "- Action-2: DOWN\n",
    "\n",
    "__NOTE__: the environment generally have DIFFERENT idea about what number represents what action, or some env may not accept numbers, but strings to represent actions! But this has no effect in our learning problem. We just make a trivial mapping to tell the game-environment to perform \"UP\" when our `action==0`.\n",
    "\n",
    "You can use the following cell to try out the code of each action used by our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "tmp_env = atari.create_atari_env('PongDeterministic-v4')\n",
    "tmp_env.reset()\n",
    "tmp_done_ = False\n",
    "tmp_steps_ = 0\n",
    "TRY_ACT = 3\n",
    "while not tmp_done_ and tmp_steps_ < 300:\n",
    "    tmp_env.render()\n",
    "    _, _, tmp_done_, _ = tmp_env.step(TRY_ACT) \n",
    "    # step is the way to commit an action\n",
    "    time.sleep(0.01)\n",
    "    tmp_steps_ += 1\n",
    "    \n",
    "tmp_env.close()\n",
    "del tmp_env\n",
    "    \n",
    "\n",
    "# NOTE: 0-STAY, 2-UP, 3-DOWN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting what we had tried so far, so we are ready to make our AI game-player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network AI playing game.\n",
    "ACTION_CODE = [0, 2, 3]\n",
    "state = env.reset()\n",
    "done = False\n",
    "steps = 0\n",
    "TRY_ACT = 3\n",
    "while not done and steps < 500:\n",
    "    env.render()\n",
    "    state_tensor = torch.from_numpy(state).unsqueeze(0)\n",
    "    features = conv_layers(state_tensor)\n",
    "    features = features.view(1, -1)\n",
    "    action_likelihood = actor_linear(features)\n",
    "    action_prob = nn.functional.softmax(action_likelihood, dim=1) \n",
    "    action = action_prob.multinomial(1)\n",
    "    \n",
    "    new_state, reward, done, _ = env.step(ACTION_CODE[action]) \n",
    "    state = new_state\n",
    "    # step is the way to commit an action\n",
    "    time.sleep(0.05)\n",
    "    steps += 1\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note although our agent looks random and stupid now, there a key difference from a really random player. \n",
    "```\n",
    "new_state, reward, done, _ = env.step(ACTION_CODE[action])\n",
    "```\n",
    "It collects observations and act ACCORDING to what has been observed in the next step. It is also aware how well/poor it has performed by collecting the `reward`. In other words, it has a brain, which opens the door of learning. \n",
    "\n",
    "So we are ready to setup our learning problem:\n",
    "* create a Python file `models.py` in our `atari_a3c` module folder.\n",
    "\n",
    "Put the definitions of convolutional representation and the decision making components (called `Actor`) and the Agent  as follows:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from .utils import weights_init\n",
    "\n",
    "\n",
    "class StateRepresentor(torch.nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(StateRepresentor, self).__init__()\n",
    "        conv1 = nn.Conv2d(in_channels, 32, 3, stride=2, padding=1)\n",
    "        conv2 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        conv3 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        conv4 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            conv1, nn.ELU(),\n",
    "            conv2, nn.ELU(),\n",
    "            conv3, nn.ELU(),\n",
    "            conv4, nn.ELU())\n",
    "        self.apply(weights_init)\n",
    "\n",
    "    def forward(self, state_tenor):\n",
    "        num_samples = state_tenor.shape[0]\n",
    "        feats = self.conv_layers(state_tenor)\n",
    "        feats = feats.view(num_samples, -1)\n",
    "        return feats\n",
    "\n",
    "\n",
    "class Actor(torch.nn.Module):\n",
    "    def __init__(self, feat_num, action_num):\n",
    "        super(Actor, self).__init__()\n",
    "        self.h_linear = nn.Linear(feat_num, 256)\n",
    "        self.a_linear = nn.Linear(256, action_num)\n",
    "\n",
    "    def forward(self, feats):\n",
    "        h = F.elu(self.h_linear(feats))\n",
    "        lp = self.a_linear(h)\n",
    "        return lp\n",
    "\n",
    "\n",
    "class Agent(torch.nn.Module):\n",
    "    def __init__(self, input_channels, feat_num, actions):\n",
    "        super(Agent, self).__init__()\n",
    "        self.repnet = StateRepresentor(input_channels)\n",
    "        self.actor = Actor(feat_num, actions)\n",
    "\n",
    "    def forward(self, state_tensor):\n",
    "        return self.actor(self.repnet(state_tensor))\n",
    "\n",
    "```\n",
    "\n",
    "Also put the playing routine into a separate file `learn.py`.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "\n",
    "def play(env, agent, first_state,\n",
    "         max_steps=20, render=False, action_code=[0, 2, 3]):\n",
    "    done = False\n",
    "    steps = 0\n",
    "    state = first_state\n",
    "    while not done and steps < max_steps:\n",
    "        if render:\n",
    "            env.render()\n",
    "\n",
    "        state_tensor = torch.from_numpy(state).unsqueeze(0)\n",
    "        action_likelihood = agent(state_tensor)\n",
    "        action_prob = nn.functional.softmax(action_likelihood, dim=1)\n",
    "        action = action_prob.multinomial(1)\n",
    "\n",
    "        new_state, reward, done, _ = env.step(action_code[action])\n",
    "        state = new_state\n",
    "        if render:\n",
    "            time.sleep(0.05)\n",
    "        steps += 1\n",
    "```\n",
    "Note I added some initialisation steps in defining the net, which are optional. Such helper functions are in the `utils.py`.\n",
    "\n",
    "Also, let's put useful functions/classes in the `atari_a3c/__init__.py` to clean up our import.\n",
    "\n",
    "Start over and check the following conceptually clean program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from atari_a3c import play, Agent, create_atari_env\n",
    "\n",
    "env = create_atari_env('PongDeterministic-v4')\n",
    "agent = Agent(input_channels=4, feat_num=288, actions=3)\n",
    "state = env.reset()\n",
    "play(env, agent, state, max_steps=50, render=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning the policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we perform learning, by \"learning\", we mean\n",
    "\n",
    "- play game using current agent\n",
    "- collect relevant information from the environment along the way, which refers to the returned information by the `env.step()` operation.\n",
    "- adjust agent parameters, so the behaviour of the agent gets improved a bit in each step\n",
    "- by _improve_, we mean to collect more `reward` in each _episode_\n",
    "- by _episode_, we mean the period from `env.reset()` to the step where the returned `done` flag is True.\n",
    "\n",
    "Immediately, we find it is necessary to enhance our `play` function a bit to collect the information for learning:\n",
    "\n",
    "We add a record book called `trajectory`, and add an entry to each list in the trajectory in each playing step\n",
    "```python\n",
    "trajectory = {'states': [],\n",
    "              'rewards': [],\n",
    "              'actions_logprob': [],\n",
    "              'actions': []}\n",
    "              \n",
    "     ...\n",
    "     \n",
    "trajectory['states'].append(state)\n",
    "trajectory['rewards'].append(reward)\n",
    "trajectory['actions'].append(action)  # extract single-num\n",
    "trajectory['actions_logprob'].append(action_logprob[0, action])\n",
    "```\n",
    "\n",
    "The information `state`, `reward` and `action` is easy to understand. You might be confused by the log-probability of an action. This is a key to our learning. We will revisit it below, for now, the caluculation is: \n",
    "\n",
    "> action_logprob = nn.functional.log_softmax(action_likelihood, dim=1)\n",
    "\n",
    "which consists of $3$ numbers corresponding to the 3 probability weights of the \"UP\", \"STAY\" and \"DOWN\" actions computed by our agent. Note\n",
    "1. For our learning, we need only one of the 3 numbers, which correspond to the action we acutally took. So we `append(action_logprob[0, action])`, the 0 refers to the first (and only) sample -- the shape of actions_logprob is $1 \\times 3$ in each step. For example, if the action randomly drawn is 1, then `action_logprob[0, 1]` is recorded.\n",
    "2. Unlike other values, such as `action`, which are simple numbers, we need to keep the `tenor` data structure. I.e. in our record, we do not only keep the result of this log-probability as a value, but also the entire computational history -- how it is computed involving states and particularly __model parameters__ is also saved. Therefore, this is the key to figure our how to adjust our model parameters.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from atari_a3c import play, Agent, create_atari_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = create_atari_env('PongDeterministic-v4')\n",
    "agent = Agent(input_channels=4, feat_num=288, actions=3)\n",
    "state = env.reset()\n",
    "trj, state, done = play(env, agent, state, max_steps=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please check `trj`. For now, it is strongly recommended to attempt to invent some own scheme to adjust the agent's parameters. Remind the typical learning procedure of a torch model is \n",
    "\n",
    "1. to allocate an `optimiser` to hold and handle all the parameters\n",
    "2. to design some __object__ to __minimise__\n",
    "3. to __back propagate__ desired change of the object\n",
    "4. to let the optimiser to update model parameters\n",
    "\n",
    "E.g. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. optimiser\n",
    "from torch.optim import Adam\n",
    "optim = Adam(agent.parameters)\n",
    "\n",
    "# 2. objective\n",
    "accumu_reward = 0\n",
    "for r_ in trj['rewards']:\n",
    "    accumu_reward += r_\n",
    "L = -accumu_reward  # we want to maximise reward, \n",
    "# but optimisers only minimising stuff, so we negate the object\n",
    "\n",
    "# 3. back-prop\n",
    "L.backward()\n",
    "\n",
    "# 4. update parameters\n",
    "optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the above scheme (and/or your trail) work? If not, what is in the way?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a more formal discussion on the difficulty to find directions to adjust the parameters, you can skip it in the first reading.\n",
    "\n",
    "__THEORY STARTS__\n",
    "\n",
    "Formally, now we are at the core of the problem: we have collected a \"trajectory\" of \n",
    "\n",
    "> $[s_0, a_0, r_0]$, $[s_1, a_1, r_1]$, ...\n",
    "\n",
    "using our current policy $\\pi_{\\theta}$. The key question is how we should adjust $\\theta$ to improve our policy? I.e. we need a direction $\\nabla_\\theta$ in the space of $\\Theta$, so if we move $\\theta$ along $\\nabla_\\theta$, hopefully our policy $\\pi_{\\theta_{New}}$ would be better at collecting rewards.\n",
    "\n",
    "In standard supervised learning, we compare the model output $\\hat{y}(\\theta)$ with some ground-truth $y$, and get a loss $L(\\theta)$. Then the back-propagation algorithm applies chain-rule to compute a direction along which to change $\\theta$ minimise $L(\\theta)$. Note we intentionally denote the dependency on parameters $\\theta$ in the former statement. \n",
    "\n",
    "However, the the above learning scheme, the objective is to maximise the reward, which is __returned by the environment__, with the model of how the world is working unknown, we cannot directly compute the influence of $\\theta$ on the reward. Moreover, although the action probability is directly influenced by $\\theta$, the action itself is the result of a stochastic process, the chain of derivation computation doesn't pass through the stochastic operation of \"drawing samples from a multinomial distribution\" to the parameters of the distribution.\n",
    "\n",
    "In a seminar [paper][1] Sutton et al. established that the influence on the __expected accumulated return__ by the parameters of the agent model has the form\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_k} = \n",
    "\\mathbf{E}_{s\\sim d^\\pi, a\\sim \\pi(\\cdot|s)}\\big[ \n",
    "  \\frac{\\partial \\pi(a|s)}{\\partial \\theta_k} A^{\\pi}(s, a)\n",
    "\\big]\n",
    "$$\n",
    "\n",
    "[1]: https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $J$ represents the target, expected total reward.\n",
    "\n",
    "* $d^\\pi$ is the __stationary probability distribution__ of the states following policy $\\pi$, which is a fancy name for the concept: \n",
    "> If one follows policy $\\pi$ (our agent), and play with the game environment infinitely many steps. She collects all states encountered, stripping the time/order of the states arriving in her collection, what the distribution of different states should be.\n",
    "\n",
    "* $a\\sim \\pi(\\cdot|s)$ is simply following $\\pi$ at each state as we had done in the steps drawing multinomial random number.\n",
    "\n",
    "* $A^\\pi(s, a)$ is a real number, representing an assessment about how good/poor if following $\\pi$ from the state-action pair $(s, a)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__THEORY ENDS__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is intuitive: instead of computing the influence of the model parameters on the total reward (because we cannot), we compute the influence of the model parameters on the action taken, and weigh the influence by a factor relating to the outcome of this agent's interaction with the environment. So\n",
    "- if an action leads to good great reward, we adjust our agent so in future, if we are at similar states, we increase the oppertunity to take the same action\n",
    "- on contrary, if an action leads to penalty, we try to avoid it in future encountering of the similar states.\n",
    "\n",
    "Note, this is the reason behind our practice of taking only the log_probability of the action that has been actualised:\n",
    "> `trajectory['actions_logprob'].append(action_logprob[0, action])`\n",
    "\n",
    "rather than\n",
    "\n",
    "> `trajectory['actions_logprob'].append(action_logprob)`\n",
    "\n",
    "because we don't know what to do with our calculation about hypothetical decisions. We can only access decisions we had committed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All analysis are theoretical so far, if we want to realise the scheme, here is a list of immediate issues and quick-dirty solutions:\n",
    "- we don't know how to compute the expectation over an unknown distribution (i.e. all possible outcomes by following our agent $\\pi$\n",
    "    - use just one trajectory to \"estimate\" (acutally, to represent) the expectation\n",
    "- we don't know the assessment of (state, action) pair if letting our agent start from there\n",
    "    - summing up received rewards until reach `done==True`, which is called an _episode_\n",
    "- (following above), what if an episode never ends, or runs so long that is effectively endless for my computer?\n",
    "    - just take some maximum steps, and cut the computation there (so you see the reason why we have the `max_step` argument in `play()`)\n",
    "    \n",
    "Let's try the simple idea:\n",
    "```python\n",
    "def estimate_policy_gradient_v0(trajectory, optim):\n",
    "    total_reward = 0\n",
    "    total_objective = 0.0\n",
    "    for r, logp in zip(reversed(trajectory['rewards']),\n",
    "                       reversed(trajectory['actions_logprob'])):  # traverse the experience\n",
    "                       # backwards, it is more convenient to get the total reward at each \n",
    "                       # time step this way\n",
    "        total_reward += r  # the accumulated reward starting from this time-step\n",
    "        # until the end of the episode\n",
    "        total_objective += total_reward * logp\n",
    "\n",
    "    total_objective = - total_objective\n",
    "    optim.zero_grad()\n",
    "    total_objective.backward()\n",
    "    optim.step()\n",
    "    \n",
    "\n",
    "def policy_gradient(env, agent):\n",
    "    optim = Adam(agent.parameters(), lr=5e-5)\n",
    "    state = env.reset()\n",
    "\n",
    "    while True:  # never stops learning!\n",
    "        trj, state, done = play(env, agent, state, max_steps=20)\n",
    "        if done:\n",
    "            state = env.reset()  # start over when game-over\n",
    "            # ... do evaluation to check progress ...\n",
    "\n",
    "        estimate_policy_gradient(trj, optim)\n",
    "```\n",
    "\n",
    "There are some useful things to do during training, such as evaluate the policy periodically to see if everything works, or save the latest model parameters from time to time:\n",
    "```python\n",
    "            if report_train_steps > report_every_n_steps:\n",
    "                # let's check the agent's performance\n",
    "                trj, _, _ = play(env, agent, state, max_steps=1000000)\n",
    "                state = env.reset() # this game has been consumed by testing, so \n",
    "                # we need to start-over again.\n",
    "                print(\"Train {}: Test score {}\".format(\n",
    "                    train_steps, sum(trj['rewards'])))\n",
    "                report_train_steps = 0\n",
    "                \n",
    "                # optional: save intermediate models\n",
    "                state_dict = agent.state_dict()\n",
    "                torch.save(state_dict,\n",
    "                           'atari_a3c/checkpoints/'\n",
    "                           'vanilla_{}.pth'.format(train_steps))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And add a testing script:\n",
    "```python\n",
    "from atari_a3c import Agent, create_atari_env\n",
    "from atari_a3c.learn import policy_gradient\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    g_env = create_atari_env('PongDeterministic-v4')\n",
    "    g_agent = Agent(input_channels=4, feat_num=288, actions=3)\n",
    "    policy_gradient(g_env, g_agent)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you follow this scheme, don't expect much. I save this version of implementation as `learn_v1.py` and `test_v1.py` respectively. You can try yourself, but on my MacBook Pro, the progress is so slow that I didn't find much happening at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main difficulty lies in the fact that we are doing stochastic estimation in a very large space using very few samples:\n",
    "- actually, the expectation over $s\\sim d^\\pi$ is performed using a single sample! (after each update, the policy $\\pi$ changes, and how often a  state is visited under $\\pi$ changes along with $\\pi$, so for each particular $\\pi$, the estimation is done using a single sample)\n",
    "- the space is large -- every possible combination of individual image pixels (4-stacked) is a member of the state space\n",
    "- the estimation of the expected future return is too crude -- we simple look forward 20 steps and see how well the policy performed during the short period\n",
    "\n",
    "So one must expect very high variance in the estimation, so much so that failure is almost certain -- And it is those challenges that make Artificial Intelligence fancinating -- how our brain manages to learn any thing despite the difficulties?\n",
    "\n",
    "We will adopt several techniques to improve our learning agent. First and foremost,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance reduction by improving advantage estimation\n",
    "\n",
    "Generally, it is relatively easy to calculate how to change the parameters to increase or decrease the chance of taking a particular action in future. However, the difficult part is to assess how good/poor the state a particular decision has led us to. And since we are consider the probability of choosing the actions, it is the relative advantage of one action over another that matters, not the absolute value. So it is proposed to introduce a critic model, which estimate the overall value of a state (in the same paper we mentioned above). \n",
    "\n",
    "By substract the average expected future return from the evaluation of each (state, action) pair, we are looking at their relative fitness to the task. This does not change the expectation, but can significantly reduce the variane of the estimation. Given the fact that we have only one sample, the method is known to speed up training.\n",
    "\n",
    "Specifically, let us allocate a critic net, which has very similar structure as the agent itself, but generates one real-number as the estimation of the long-term return starting from some state $s$ following a policy $\\pi$.\n",
    "```python\n",
    "class Critic(torch.nn.Module):\n",
    "    def __init__(self, feat_num):\n",
    "        super(Critic, self).__init__()\n",
    "        self.h_linear = nn.Linear(feat_num, 256)\n",
    "        self.q_linear = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, feats):\n",
    "        h = F.elu(self.h_linear(feats))\n",
    "        v = self.q_linear(h)\n",
    "        return v\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When playing and collecting information, the critic's assessment on each state is saved as well\n",
    "```\n",
    "def play(...):\n",
    "    ...\n",
    "    trajectory = {...\n",
    "                  'critic_values': []}\n",
    "    while not done and steps < max_steps:\n",
    "        ...        \n",
    "        action_likelihood, critic_val = agent(state_tensor)\n",
    "        ...\n",
    "        trajectory['critic_values'].append(critic_val)\n",
    "```\n",
    "\n",
    "When training the model, we made several updates in the gradient estimation:\n",
    "- __[A]__ compute the advantage by substracting the predicted value from the accumulated return\n",
    "\n",
    "- __[B]__ including training for the critic, i.e. by requiring the estimation value of a state to be close to the actual outcome\n",
    "\n",
    "- __[C]__ including a discount factor $\\gamma$, so when accumulating the future reward, we are looking at a finite time horizon (when a reward is too remote in time, you may not want it to affect the current decision -- such influence, if arises from data, is more likely by coincidence than reflecting any patterns\n",
    "\n",
    "```python\n",
    "def estimate_policy_gradient(trajectory, optim, gamma=0.99):\n",
    "    total_reward = 0\n",
    "    total_objective = 0.0\n",
    "    actor_objective = 0.0\n",
    "    critic_objective = 0.0\n",
    "    for r, logp, cv in zip(reversed(trajectory['rewards']),\n",
    "                       reversed(trajectory['actions_logprob']),\n",
    "                       reversed(trajectory['critic_values'])):\n",
    "        # the accumulated reward starting from this time-step\n",
    "        # until the end of the episode\n",
    "        total_reward = total_reward*gamma + r  # [C]\n",
    "        advantage = total_reward - cv  # [A]\n",
    "        actor_objective += advantage.detach() * logp  # for actor,\n",
    "        # we treat advantage as a coefficient, not some value to adjust\n",
    "\n",
    "        critic_objective += advantage ** 2  # [B]\n",
    "\n",
    "    total_objective = critic_objective - actor_objective\n",
    "    optim.zero_grad()\n",
    "    total_objective.backward()\n",
    "    optim.step()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm worked, but it is very slow. You can see it made progress (such as stay in the game for longer), but it took very long to learn any winning policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance reduction by parallel learning\n",
    "\n",
    "As the current Monte Carlo estimation scheme employs only one sample to represent the expectation, an obvious improvement is to collect more samples. However in the reinforcement learning, data are generated within the training process. So the process is inherently serial. To have more samples, a natural way is to run multiple game-sessions simultaneously, and make a gradient estimation in each of the sessions. Then we use the average of the estimated gradients to update our model. \n",
    "\n",
    "There are mainly two ways to implement the scheme:\n",
    "1. Server-Client Mode\n",
    "    - Build a \"master-model\", which is like a centralised server. \n",
    "    - Each learning worker uses the up-to-date model \"pulled\" from the server, generate trajectory data.\n",
    "    - Each worker applies the gradient it computed to the server model. \n",
    "\n",
    "2. Shared Model Mode\n",
    "    - Create a shared model\n",
    "    - Each learning worker use this shared model \n",
    "    - The individually computed gradients are applied to the shared model\n",
    "    \n",
    "Traditionally, method-1 is easier to implement, but fortunately, `pytorch` provides very nice parallelisation method to automatically handle the issues such one worker is using the shared model to compute the action probability and the other is trying to update the parameters of the nets. So we choose the second one. \n",
    "\n",
    "Then change of code is minimal. \n",
    "\n",
    "```python\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "def start_learning(num_workders, env_name):\n",
    "    shared_agent = Agent(input_channels=4, feat_num=288, actions=3)\n",
    "    shared_agent.share_memory()  # so the model's parameters is managed by\n",
    "    # pytorch to avoid access conflicts.\n",
    "    processes = []\n",
    "    for rank in range(num_workders):\n",
    "        p = mp.Process(target=policy_gradient,\n",
    "                       args=(rank, env_name, shared_agent))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "\n",
    "    # Separate evaluation process\n",
    "    p = mp.Process(target=evaluate_policy, args=(env_name, shared_agent))\n",
    "    p.start()\n",
    "    processes.append(p)\n",
    "\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "```\n",
    "\n",
    "Note the multiprocess learning also allows me to move evaluation (along with all the scheduling of evaluation, saving training progress, etc. etc.) to a separate process. So we can afford much nicer, more informative evaluation along the training without introducing clutters and bugs in the learning code. The new learner is `learn_mp.py`, and a simple test is via `test_02.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The efficiency is better, but this program still very slow to converge and with strong instability.\n",
    "\n",
    "If you are interested, please consider your own method to improve (of course, based on existing techniques, this is a viable path for Assignment 2 and 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot going on in this challenging and exciting area. We will introduce more works in the class following the stuvac (and put a reference list here after)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further help your study, below is a glossary that might be useful when studying the literature. Also, a faster working implementation of the A3C algorithm is  provided [here](https://github.com/junjy007/aifoundation/tree/master/utils/a3c)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Math Glossary\n",
    "Some math representation of useful concepts are:\n",
    "\n",
    "- $s_t$: the state observed at time $t$. This is usually but NOT always the stuff returned to the agent at the time taking actions. The most prominent exception is the screen-image-based states in our video-game playing examples. One applies some simple preprocessing to the states.\n",
    "\n",
    "- $a_t$: the action taken at time $t$. Generally, it is an integer $\\{0, 1, ..., K-1\\}$ if there are $K$ different actions. Keep in mind that the actual action could be represented differently, such as \"press A-button\". For a decision making agent, given all possible action choices, choosing actions is eqivalent to choosing the indexes.\n",
    "\n",
    "- $r_t$: the immediate reward received at time $t$. Note some authors used to let $r_t$ refer to the reward received __after__ taking action $a_t$ in state $s_t$, while others take $r_t$ as the reward received __at the beginning__ at time $t$, after taking action $a_{t-1}$ in state $s_{t-1}$. In whatever way, the procedure: in $s_t$ taking action $a_t$ according to some policy $\\pi$ arriving the next state $s_{t+1}$ and receiving a reward $r_{t}$ (or $r_{t+1}$ subject to your choice of denotation) is called a __transition step__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\pi(\\cdot|s)$, given the state $s$, a policy $\\pi(a|s)$ (not to be confused with the $\\pi\\approx3.1416$) assigns a non-negative real number to each action -- it dictates the possibility of choosing the action given $s$. If a policy is deterministic, rather than stochastic, it can attribute all probabilities to one particular action, so that the corresponding $\\pi(a|s)=1$ and $\\pi(a'|s)=0$ for all other actions $a'$.\n",
    "\n",
    "- $Q^\\pi(s, a)$, evaluation of __long term__ return for taking action $a$ in state $s$. Since it considers future effects, it relies on the on-going policy, $\\pi$. Note taking $a$ at the current state $s$, the very first step of this evaluation is not necessarily with respect to $\\pi$. Consider this $Q$-evaluation as answering a hypothetical question: what the long term reward would have been if she took action $a$ at $s$ and followed $\\pi$ henceafter. Of course if this evaluation is known, it is wise to take the action maximising this $Q$ at each $s$.\n",
    "\n",
    "- Note in neural network implementation, $Q$- and $\\pi$-nets share the same structure: map states to $K$ numbers, where $K$ is the number of actions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
